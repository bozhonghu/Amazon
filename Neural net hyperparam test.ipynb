{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcess\n",
    "import random\n",
    "\n",
    "\n",
    "# Seed the random number generator:\n",
    "np.random.seed(1)\n",
    "\n",
    "def load_data(filename, skiprows = 1):\n",
    "    \"\"\"\n",
    "    Function loads data stored in the file filename and returns it as a numpy ndarray.\n",
    "    \n",
    "    Inputs:\n",
    "        filename: given as a string.\n",
    "        \n",
    "    Outputs:\n",
    "        Data contained in the file, returned as a numpy ndarray\n",
    "    \"\"\"\n",
    "    return np.loadtxt(filename, skiprows=skiprows, delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_data('training_data.txt')\n",
    "y = X[:, 0]\n",
    "X = X[:, 1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_short = X[:10]\n",
    "y_short = y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1000) (10,)\n",
      "(20000, 1000) (20000,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_short), np.shape(y_short))\n",
    "print(np.shape(X), np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using grid to optimize hyper-params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_tuning_model(X_train, y_train, X_test, y_test, h_params):\n",
    "    '''\n",
    "    Architecture stays constant except for toggling batch normalization\n",
    "    \n",
    "    h_params: {'layer_sizes' : [size1, size2, size3], \n",
    "                'dropouts' : [d1, d2]\n",
    "                }\n",
    "    '''\n",
    "    \n",
    "    ## Create your own model here given the constraints in the problem\n",
    "    model = Sequential() # Use np.reshape instead of this in hw\n",
    "        \n",
    "    model.add(Dense(h_params['layer_sizes'][0], input_dim = 1000))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(h_params['dropouts'][0]))\n",
    "    \n",
    "    model.add(Dense(h_params['layer_sizes'][1]))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(h_params['dropouts'][1]))\n",
    "    \n",
    "    model.add(Dense(h_params['layer_sizes'][2]))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    ## Printing a summary of the layers and weights in your model\n",
    "    #model.summary()\n",
    "    \n",
    "    ## In the line below we have specified the loss function as 'mse' (Mean Squared Error) because in the above code we did not one-hot encode the labels.\n",
    "    ## In your implementation, since you are one-hot encoding the labels, you should use 'categorical_crossentropy' as your loss.\n",
    "    ## You will likely have the best results with RMS prop or Adam as your optimizer.  In the line below we use Adadelta\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='Adam', metrics=['accuracy'])\n",
    "    \n",
    "    fit = model.fit(X_train, y_train, batch_size=128, epochs=2,\n",
    "        verbose=0)\n",
    "\n",
    "    ## Printing the accuracy of our model, according to the loss function specified in model.compile above\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "#     print('Test score:', score[0])\n",
    "#     print('Test accuracy:', score[1])\n",
    "    \n",
    "    return score[1], model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def kfold(X, y, h_params):\n",
    "    \n",
    "#     print(np.shape(X))\n",
    "#     print(np.shape(y))\n",
    "    \n",
    "    kf = KFold(n_splits = 8, shuffle=True)\n",
    "    scores = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        y_train = to_categorical(y_train)\n",
    "        y_test = to_categorical(y_test)\n",
    "\n",
    "        scores.append(make_tuning_model(X_train, y_train, X_test, y_test, h_params)[0])\n",
    "\n",
    "#     print('validation acc: ' + str(np.mean(scores)))\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations left: 4\n",
      "Iterations left: 3\n",
      "Iterations left: 2\n",
      "Iterations left: 1\n",
      "hyper params           :               Validation accuracy\n",
      "{'layer_sizes': (1500, 300, 200), 'dropouts': (0.45, 0.33)}: 0.84755\n",
      "{'layer_sizes': (2000, 500, 100), 'dropouts': (0.45, 0.33)}: 0.8472500000000001\n",
      "{'layer_sizes': (1500, 300, 200), 'dropouts': (0.4, 0.3)}: 0.847\n",
      "{'layer_sizes': (2000, 500, 100), 'dropouts': (0.4, 0.3)}: 0.842\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "layer_sizes = [(1500, 300, 200), (2000, 500, 100)]\n",
    "dropouts = [(.4, .3), (.45, .33)]\n",
    "\n",
    "validation_accs = {}\n",
    "\n",
    "iters = len(layer_sizes) * len(dropouts)\n",
    "\n",
    "for layer_size in layer_sizes:\n",
    "    for dropout in dropouts:\n",
    "        print('Iterations left: ' + str(iters))\n",
    "        iters -= 1\n",
    "          \n",
    "        h_params = {'layer_sizes' : layer_size, 'dropouts' : dropout}\n",
    "        \n",
    "        val_acc = kfold(X, y, h_params)\n",
    "\n",
    "        validation_accs[val_acc] = h_params\n",
    "    \n",
    "scoress = list(validation_accs.keys())\n",
    "scoress.sort(reverse=True)\n",
    "\n",
    "print('hyper params           :               Validation accuracy')\n",
    "for s in scoress:\n",
    "    print(str(validation_accs[s]) + ': ' + str(s))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 1000)\n",
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X))\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 14s 680us/step - loss: 0.4426 - acc: 0.7948\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 12s 578us/step - loss: 0.3034 - acc: 0.8681\n",
      "Test score: 0.1895963114976883\n",
      "Test accuracy: 0.92885\n"
     ]
    }
   ],
   "source": [
    "X_test = load_data('test_data.txt')\n",
    "X = load_data('training_data.txt')\n",
    "y = X[:, 0]\n",
    "y = to_categorical(y)\n",
    "X = X[:, 1:]\n",
    "m = make_model(X, y, X, y)[1]\n",
    "pred = m.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_pred(pred):\n",
    "    result = [1 if i[0] == 0 else 0 for i in np.round(pred)]\n",
    "    with open(\"result.txt\", \"w\") as f:\n",
    "        f.write(\"Id,Prediction\\n\") \n",
    "        for i in range(1, len(result) + 1):\n",
    "            f.write(str(i) + \",\" + str(result[i-1]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_pred(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
